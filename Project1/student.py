# -*- coding: utf-8 -*-
"""Student

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12B1vA2dFl8zmXA391-1t_rSGlBIErLtU
"""

from pyspark import SparkConf,SparkContext
conf=SparkConf().setAppName("Test")
sc=SparkContext(conf=conf)

df=sc.textFile("/content/drive/MyDrive/ColabNotebooks/project1/StudentData.csv")

header=df.first()
df=df.filter(lambda x:x!=header)

count=df1.count()

df2=df.map(lambda x:x.split(","))

df3=df2.map(lambda x:(x[1],int(x[5])))
totalmarks=df3.reduceByKey(lambda x,y:x+y)

passed=df2.map(lambda x:int(x[5])).filter(lambda x:x>50).count()
failed=df2.filter(lambda x:int(x[5])<=50).count()
print(passed,failed)

total_enrollments=df2.map(lambda x:(x[3],1)).reduceByKey(lambda x,y:x+y).collect()
print(total_enrollments)

total_marks_percourse=df2.map(lambda x:(x[3],int(x[5]))).reduceByKey(lambda x,y:x+y)
total_marks_percourse.collect()

average=df2.map(lambda x:(x[3],(int(x[5]),1))).reduceByKey(lambda x,y:(x[0]+y[0],x[1]+y[1])).map(lambda x:(x[0],x[1][0]/x[1][1]))
average.collect()

minimum=df2.map(lambda x:(x[3],(int(x[5])))).reduceByKey(lambda x,y:x if x<y else y)
maximum=df2.map(lambda x:(x[3],(int(x[5])))).reduceByKey(lambda x,y:x if x>y else y)
print("Minimum:",minimum.collect())
print("Maximum:",maximum.collect())

Avg_age=df2.map(lambda x:(x[1],(int(x[0]),1))).reduceByKey(lambda x,y:(x[0]+y[0],x[1]+y[1])).map(lambda x:(x[0],x[1][0]/x[1][1]))
Avg_age.collect()